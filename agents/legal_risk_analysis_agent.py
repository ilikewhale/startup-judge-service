from dotenv import load_dotenv
from langchain_teddynote import logging
from langchain_opentutorial.rag.pdf import PDFRetrievalChain
from langchain_core.tools.retriever import create_retriever_tool
from langchain_core.prompts import PromptTemplate
from typing import Annotated, Sequence, TypedDict, Literal, Dict
from langchain_core.messages import BaseMessage, HumanMessage
from langgraph.graph.message import add_messages
from langchain_core.output_parsers import StrOutputParser
from pydantic import BaseModel, Field
from langchain_openai import ChatOpenAI
from langgraph.prebuilt import tools_condition
from langchain_teddynote.models import get_model_name, LLMs
from langgraph.graph import END, StateGraph, START
from langchain_teddynote.tools.tavily import TavilySearch
from langchain_teddynote.messages import stream_graph
import asyncio
import os

load_dotenv()
logging.langsmith("CH15-Agentic-RAG-Legal")

# ë²•ì  ë¦¬ìŠ¤í¬ ë¶„ì„ ìƒíƒœ ì •ì˜ 
class LegalRiskAgentState(TypedDict):
    messages: Annotated[Sequence[BaseMessage], add_messages]
    company: str
    domain: str
    tech_summary: str  # ê¸°ìˆ  ìš”ì•½ ì •ë³´
    country: str        # ì§€ì—­ ì •ë³´

    legal_assessments: Dict[str, str]


# ëª¨ë¸ ì´ë¦„ ì„¤ì •
MODEL_NAME = get_model_name(LLMs.GPT4)

# PDF íŒŒì¼ë¡œë¶€í„° ê²€ìƒ‰ ì²´ì¸ ìƒì„±
def create_pdf_retriever():
    file_path = ["data/2023 êµ­ë‚´ì™¸ AI ê·œì œ ë° ì •ì±… ë™í–¥.pdf", "data/ì¸ê³µì§€ëŠ¥(AI) ê´€ë ¨ êµ­ë‚´ì™¸ ë²•ì œ ë™í–¥.pdf"]
    pdf_file = PDFRetrievalChain(file_path).create_chain()
    pdf_retriever = pdf_file.retriever
    
    # PDF ë¬¸ì„œë¥¼ ê¸°ë°˜ìœ¼ë¡œ ê²€ìƒ‰ ë„êµ¬ ìƒì„±
    retriever_tool = create_retriever_tool(
        pdf_retriever,
        "legal_pdf_retriever",
        "Search and return information about AI legal and regulatory frameworks from the PDF files. They contain essential information on AI regulations, policies, and legal trends relevant for AI startups. The documents are focused on both domestic and international AI legal frameworks.",
        document_prompt=PromptTemplate.from_template(
            "<document><context>{page_content}</context><metadata><source>{source}</source><page>{page}</page></metadata></document>"
        ),
    )
    
    return retriever_tool

# ë°ì´í„° ëª¨ë¸ ì •ì˜
class grade(BaseModel):
    """A binary score for relevance checks"""
    binary_score: str = Field(
        description="Response 'yes' if the document is relevant to the legal/regulatory question or 'no' if it is not."
    )

# ë¬¸ì„œ ê´€ë ¨ì„± í‰ê°€ í•¨ìˆ˜ (ì¡°ê±´ë¶€ ì—£ì§€ì—ì„œ ì‚¬ìš©)
def grade_documents(state: LegalRiskAgentState) -> str:
    # LLM ëª¨ë¸ ì´ˆê¸°í™”
    model = ChatOpenAI(temperature=0, model=MODEL_NAME, streaming=True)

    # êµ¬ì¡°í™”ëœ ì¶œë ¥ì„ ìœ„í•œ LLM ì„¤ì •
    llm_with_tool = model.with_structured_output(grade)

    # í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿ ì •ì˜
    prompt = PromptTemplate(
        template="""You are a legal expert assessing relevance of a retrieved document to an AI startup's legal/regulatory risk question. \n 
        Here is the retrieved document: \n\n {context} \n\n
        Here is the question about AI legal/regulatory risks: {question} \n
        If the document contains keyword(s) or semantic meaning related to the legal/regulatory question, grade it as relevant. \n
        Give a binary score 'yes' or 'no' score to indicate whether the document is relevant to the question.""",
        input_variables=["context", "question"],
    )

    # llm + tool ë°”ì¸ë”© ì²´ì¸ ìƒì„±
    chain = prompt | llm_with_tool

    # í˜„ì¬ ìƒíƒœì—ì„œ ë©”ì‹œì§€ ì¶”ì¶œ
    messages = state["messages"]
    last_message = messages[-1]
    question = messages[0].content

    # ê²€ìƒ‰ëœ ë¬¸ì„œ ì¶”ì¶œ
    retrieved_docs = last_message.content

    # ê´€ë ¨ì„± í‰ê°€ ì‹¤í–‰
    scored_result = chain.invoke({"question": question, "context": retrieved_docs})

    # ê´€ë ¨ì„± ì—¬ë¶€ ì¶”ì¶œ
    score = scored_result.binary_score

    # ê´€ë ¨ì„± ì—¬ë¶€ì— ë”°ë¥¸ ê²°ì •
    if score == "yes":
        return "generate"
    else:
        print(score)
        return "rewrite"

# ì´ˆê¸° ì§ˆì˜ ì²˜ë¦¬ ë…¸ë“œ
def initial_query(state: LegalRiskAgentState):
    print("\nğŸŸ¢ [initial_query] ê¸°ì—… ì •ë³´ ê¸°ë°˜ ì§ˆë¬¸ ìƒì„± ì¤‘...")
    
    company = state['company']
    domain = state['domain']
    tech_summary = state['tech_summary']
    country = state['country']
    
    # ê¸°ì—…, ì‚°ì—…, ê¸°ìˆ  ìš”ì•½, ì§€ì—­ì„ ëª¨ë‘ ê³ ë ¤í•œ ì§ˆë¬¸ ìƒì„±
    question = f"{company}ì€(ëŠ”) {country}ì— ìœ„ì¹˜í•œ {domain} ë¶„ì•¼ AI ìŠ¤íƒ€íŠ¸ì—…ìœ¼ë¡œ, ë‹¤ìŒ ê¸°ìˆ ì„ í™œìš©í•©ë‹ˆë‹¤: '{tech_summary}'. ì´ ê¸°ìˆ ê³¼ ì‚°ì—… ë¶„ì•¼ë¥¼ ê³ ë ¤í–ˆì„ ë•Œ í•´ë‹¹ ì§€ì—­ì˜ ë²•ì /ê·œì œ ë¦¬ìŠ¤í¬ëŠ” ë¬´ì—‡ì¸ê°€?"
    
    print(f"â¤ ìƒì„±ëœ ì§ˆë¬¸: {question}")
    return {"messages": [HumanMessage(content=question)]}

# ë¬¸ì„œ ê²€ìƒ‰ ë…¸ë“œ 
def pdf_retrieval(state: LegalRiskAgentState):
    print("\nğŸ“„ [pdf_retrieval] PDF ê¸°ë°˜ ë²•ë¥  ë¬¸ì„œ ê²€ìƒ‰ ì‹œì‘")
    messages = state["messages"]
    question = messages[-1].content
    retriever_tool = create_pdf_retriever()
    results = retriever_tool.invoke({"query": question})
    print("âœ… ê²€ìƒ‰ ì™„ë£Œ - ê´€ë ¨ ë¬¸ì„œ ìš”ì•½ ë°˜í™˜")
    return {"messages": [HumanMessage(content=results)]}

# ì§ˆì˜ ì¬ì‘ì„± ë…¸ë“œ
def rewrite(state: LegalRiskAgentState):
    print("\nâœï¸ [rewrite] ë²•ì  ì§ˆë¬¸ì´ ë¬¸ì„œì™€ ê´€ë ¨ ì—†ìœ¼ë¯€ë¡œ ì§ˆì˜ ì¬ì‘ì„± ì‹œì‘")
    # í˜„ì¬ ìƒíƒœì—ì„œ ë©”ì‹œì§€ ì¶”ì¶œ
    messages = state["messages"]
    # ì›ë˜ ì§ˆë¬¸ ì¶”ì¶œ
    question = messages[0].content
    company = state["company"]
    domain = state["domain"]
    tech_summary = state["tech_summary"]
    country = state["country"]

    # ëª¨ë“  ìš”ì†Œë¥¼ ê³ ë ¤í•œ ì§ˆë¬¸ ê°œì„ ì„ ìœ„í•œ í”„ë¡¬í”„íŠ¸ êµ¬ì„±
    msg = [
        HumanMessage(
            content=f""" \n 
    Look at the input question about AI legal/regulatory risks for {company} in the {domain} domain in {country} country using technology: '{tech_summary}'.
    Try to reason about the underlying semantic intent / meaning. \n 
    Here is the initial question:
    \n ------- \n
    {question} 
    \n ------- \n
    Formulate an improved question that focuses on specific legal/regulatory frameworks, compliance requirements, or potential legal risks
    for this AI startup, considering their specific technology, domain, and country: """,
        )
    ]

    # LLM ëª¨ë¸ë¡œ ì§ˆë¬¸ ê°œì„ 
    model = ChatOpenAI(temperature=0, model=MODEL_NAME, streaming=True)
    # Query-Transform ì²´ì¸ ì‹¤í–‰
    response = model.invoke(msg)

    # ì¬ì‘ì„±ëœ ì§ˆë¬¸ ë°˜í™˜
    print(f"ğŸ†• ì¬ì‘ì„±ëœ ì§ˆë¬¸: {response.content.strip()[:100]}...")
    return {"messages": [response]}

# Web Search ë…¸ë“œ
def web_search(state: LegalRiskAgentState):
    print("\nğŸŒ [web_search] ì›¹ ê¸°ë°˜ ë³´ì¡° ë²•ë¥  ì •ë³´ ê²€ìƒ‰ ì‹œì‘")
    tavily_tool = TavilySearch()
    
    # ìˆ˜ì •ëœ ë¶€ë¶„: messagesì—ì„œ ë‚´ìš© ì¶”ì¶œ
    messages = state["messages"]
    search_query = messages[-1].content
    
    company = state["company"]
    domain = state["domain"]
    tech_summary = state["tech_summary"]
    country = state["country"]
    
    # ê²€ìƒ‰ ì¿¼ë¦¬ì— ê¸°ì—… ì •ë³´, ê¸°ìˆ  ìš”ì•½, ì§€ì—­ ì •ë³´ ì¶”ê°€
    enhanced_query = f"{search_query} {company} {domain} {tech_summary} {country} AI ìŠ¤íƒ€íŠ¸ì—… ë²•ì  ê·œì œ"

    search_result = tavily_tool.search(
        query=enhanced_query,  # ê²€ìƒ‰ ì¿¼ë¦¬
        topic="legal",     # ë²•ë¥  ì£¼ì œë¡œ ë³€ê²½
        max_results=3,       # ìµœëŒ€ ê²€ìƒ‰ ê²°ê³¼
        format_output=True,  # ê²°ê³¼ í¬ë§·íŒ…
    )
    print("âœ… ì›¹ ê²€ìƒ‰ ì™„ë£Œ - ìš”ì•½ ë‚´ìš© ë°˜í™˜")
    return {"messages": [HumanMessage(content=search_result)]}


# ë²•ì  ë¶„ì„ ë…¸ë“œ 
def analyze(state: LegalRiskAgentState):
    print("\nğŸ§  [analyze] ë¬¸ì„œ ê¸°ë°˜ ë²•ì  ë¦¬ìŠ¤í¬ ë¶„ì„ ì‹¤í–‰")
    # í˜„ì¬ ìƒíƒœì—ì„œ ë©”ì‹œì§€ ì¶”ì¶œ
    messages = state["messages"]
    # ì›ë˜ ì§ˆë¬¸ ì¶”ì¶œ
    question = messages[0].content
    
    # ê¸°ì—… ì •ë³´ ê°€ì ¸ì˜¤ê¸°
    company = state["company"]
    domain = state["domain"]
    tech_summary = state["tech_summary"]
    country = state["country"]

    # ê°€ì¥ ë§ˆì§€ë§‰ ë©”ì‹œì§€ ì¶”ì¶œ (ê²€ìƒ‰ ê²°ê³¼)
    docs = messages[-1].content
    
    # ë””ë²„ê·¸ ë©”ì‹œì§€ ì¶”ê°€
    print(f"ê¸°ì—…: {company}")
    print(f"ì‚°ì—…: {domain}")
    print(f"ê¸°ìˆ  ìš”ì•½: {tech_summary}")
    print(f"ì§€ì—­: {country}")
    print(f"ì§ˆë¬¸: {question}")
    print(f"ë¬¸ì„œ ê¸¸ì´: {len(docs) if docs else 0}ì")

    # RAG í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿ ì •ì˜ - ê¸°ìˆ  ìš”ì•½ê³¼ ì§€ì—­ ì •ë³´ ì¶”ê°€
    prompt = PromptTemplate(
        template="""You are a legal expert specialized in AI regulations and policies for startups. 
        Use the following pieces of context to answer the question at the end about {company} in the {domain} domain 
        located in {country} and using the following technology: '{tech_summary}'.
        
        If you don't know the answer, just say you don't know. 
        Don't try to make up an answer.
        
        Always structure your response in the following format: (ë°˜ë“œì‹œ í•œêµ­ì–´ë¡œ ë‹µë³€)
        
        1. ë²•ë¥ /ê·œì œ ë¶„ì„: 
           - {country} ì§€ì—­ì— íŠ¹í™”ëœ ë²•ì /ê·œì œ ê³ ë ¤ì‚¬í•­
           - {domain} ì‚°ì—…ì— ì ìš©ë˜ëŠ” íŠ¹ë³„ ê·œì œ ì‚¬í•­
           - {tech_summary} ê¸°ìˆ ê³¼ ê´€ë ¨ëœ íŠ¹ì • ë²•ì  ì´ìŠˆ
        
        2. ì ì¬ì  ë¦¬ìŠ¤í¬: 
           - ì´ AI ìŠ¤íƒ€íŠ¸ì—…ì´ ì§ë©´í•  ìˆ˜ ìˆëŠ” êµ¬ì²´ì ì¸ ë²•ì  ë¦¬ìŠ¤í¬
           - ê¸°ìˆ  íŠ¹ì„±ìœ¼ë¡œ ì¸í•œ ì¶”ê°€ì ì¸ ë¦¬ìŠ¤í¬
           - ì§€ì—­ ê·œì œë¡œ ì¸í•œ íŠ¹ë³„ ê³ ë ¤ì‚¬í•­
        
        3. ê·œì • ì¤€ìˆ˜ ê¶Œì¥ì‚¬í•­: 
           - ê·œì • ì¤€ìˆ˜ë¥¼ ìœ„í•œ ì‹¤ì§ˆì ì¸ ë‹¨ê³„ ì œì•ˆ
           - ê¸°ìˆ  ê°œë°œ ë° ë°°í¬ ì‹œ ê³ ë ¤í•´ì•¼ í•  ì‚¬í•­
           - ë²•ì  ë¦¬ìŠ¤í¬ ìµœì†Œí™”ë¥¼ ìœ„í•œ ì „ëµ
        
        4. êµ­ì œì  ê³ ë ¤ì‚¬í•­: 
           - í•´ë‹¹ë˜ëŠ” ê²½ìš° ê´€ë ¨ êµ­ì œ í”„ë ˆì„ì›Œí¬
           - êµ­ê°€ê°„ ë°ì´í„° ì´ì „, ì‚¬ì—… í™•ì¥ ì‹œ ê³ ë ¤í•´ì•¼ í•  ì 
        
        {context}
        
        Question: {question}
        
        Helpful Answer:""",
        input_variables=["context", "question", "company", "domain", "tech_summary", "country"],
    )

    # LLM ëª¨ë¸ ì´ˆê¸°í™”
    llm = ChatOpenAI(model_name=MODEL_NAME, temperature=0, streaming=True)

    # RAG ì²´ì¸ êµ¬ì„±
    rag_chain = prompt | llm | StrOutputParser()

    try:
        # ë‹µë³€ ìƒì„± ì‹¤í–‰
        response = rag_chain.invoke({
            "context": docs, 
            "question": question,
            "company": company,
            "domain": domain,
            "tech_summary": tech_summary,
            "country": country
        })
        print("âœ… ë¶„ì„ ì™„ë£Œ - ìš”ì•½ ë³´ê³  ìƒì„±")
        return {"messages": [HumanMessage(content=response)]}
    
    except Exception as e:
        print(f"ì˜¤ë¥˜ ë°œìƒ: {e}")
        error_msg = f"ì‘ë‹µ ìƒì„± ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}"
        return {"messages": [HumanMessage(content=error_msg)]}

# ë²•ì /ê·œì œ ë¦¬ìŠ¤í¬ ë¶„ì„ ê²°ê³¼ ì²˜ë¦¬ ë…¸ë“œ
def analyze_legal_risks(state: LegalRiskAgentState):
    print("\nğŸ“Š [analyze_legal_risks] ìµœì¢… í‰ê°€ ë‚´ìš© ì €ì¥")
    company = state["company"]
    messages = state["messages"]
    legal_assessment = messages[-1].content
    
    print(f"ğŸ“ ì €ì¥ëœ í‰ê°€: {legal_assessment[:100]}...")
    return {
        "legal_assessments": {company: legal_assessment}
    }

# í…Œí¬ë†€ë¡œì§€ ë¦¬ìŠ¤í¬ ë¶„ì„ ë…¸ë“œ (ì¶”ê°€)
def tech_risk_analysis(state: LegalRiskAgentState):
    print("\nğŸ” [tech_risk_analysis] ê¸°ìˆ  íŠ¹í™” ë²•ì  ë¦¬ìŠ¤í¬ ë¶„ì„")
    
    # ê¸°ì—… ì •ë³´ ê°€ì ¸ì˜¤ê¸°
    company = state["company"]
    domain = state["domain"]
    tech_summary = state["tech_summary"]
    country = state["country"]
    
    # ì´ì „ ë©”ì‹œì§€ì—ì„œ ë²•ì  ë¶„ì„ ê²°ê³¼ ê°€ì ¸ì˜¤ê¸°
    messages = state["messages"]
    legal_analysis = messages[-1].content
    
    # ê¸°ìˆ  íŠ¹í™” í”„ë¡¬í”„íŠ¸ êµ¬ì„±
    msg = [
        HumanMessage(
            content=f"""ë‹¹ì‹ ì€ AI ê¸°ìˆ  ë° ê·œì œ ì „ë¬¸ê°€ì…ë‹ˆë‹¤. ë‹¤ìŒ ì •ë³´ë¥¼ ë°”íƒ•ìœ¼ë¡œ {company}ì˜ ê¸°ìˆ ì´ ì´ˆë˜í•  ìˆ˜ ìˆëŠ” êµ¬ì²´ì ì¸ ë²•ì /ê·œì œ ë¦¬ìŠ¤í¬ë¥¼ ë¶„ì„í•´ì£¼ì„¸ìš”:

ê¸°ì—…: {company}
ì‚°ì—…: {domain}
ì§€ì—­: {country}
ê¸°ìˆ  ìš”ì•½: {tech_summary}

ì´ì „ ë²•ì  ë¶„ì„:
{legal_analysis}

ì´ì œ ê¸°ìˆ ì˜ íŠ¹ì„±ì— ì´ˆì ì„ ë§ì¶”ì–´ ë‹¤ìŒ ì‚¬í•­ì„ ë¶„ì„í•´ì£¼ì„¸ìš”:

1. ì´ íŠ¹ì • ê¸°ìˆ ì´ í˜„ì¬ ê·œì œ í™˜ê²½ì—ì„œ ì–´ë–¤ ê³ ìœ í•œ ë²•ì  ë¬¸ì œë¥¼ ì•¼ê¸°í•  ìˆ˜ ìˆëŠ”ì§€ 
2. ì´ ê¸°ìˆ ì´ í•´ë‹¹ ì‚°ì—… ë‚´ì—ì„œ ê·œì œ ì ìš©ì„ ë°›ì„ ë•Œì˜ íŠ¹ë³„í•œ ê³ ë ¤ì‚¬í•­
3. ì´ ê¸°ìˆ ì´ í–¥í›„ ì§ë©´í•  ìˆ˜ ìˆëŠ” ì ì¬ì ì¸ ê·œì œ ë³€í™”
4. ì´ ê¸°ìˆ ì˜ ë„ì… ë° í™œìš©ì— ë”°ë¥¸ êµ¬ì²´ì ì¸ ì¤€ìˆ˜ ì „ëµ

ë‹µë³€ì€ ë°˜ë“œì‹œ í•œêµ­ì–´ë¡œ í•´ì£¼ì„¸ìš”."""
        )
    ]
    
    # LLM ëª¨ë¸ë¡œ ê¸°ìˆ  ë¦¬ìŠ¤í¬ ë¶„ì„
    model = ChatOpenAI(temperature=0, model=MODEL_NAME, streaming=True)
    response = model.invoke(msg)
    
    print("âœ… ê¸°ìˆ  íŠ¹í™” ë¦¬ìŠ¤í¬ ë¶„ì„ ì™„ë£Œ")
    return {"messages": [response]}

# ì¢…í•© ë¶„ì„ ë° ê¶Œì¥ì‚¬í•­ ë…¸ë“œ (ì¶”ê°€)
def comprehensive_analysis(state: LegalRiskAgentState):
    print("\nğŸ“‘ [comprehensive_analysis] ì¢…í•© ë¶„ì„ ë° ê¶Œì¥ì‚¬í•­ ì‘ì„±")
    
    # ê¸°ì—… ì •ë³´ ê°€ì ¸ì˜¤ê¸°
    company = state["company"]
    domain = state["domain"]
    tech_summary = state["tech_summary"]
    country = state["country"]
    
    # ì´ì „ ë¶„ì„ ê²°ê³¼ ê°€ì ¸ì˜¤ê¸°
    messages = state["messages"]
    legal_analysis = messages[-2].content  # ë²•ì  ë¶„ì„ ê²°ê³¼
    tech_analysis = messages[-1].content   # ê¸°ìˆ  ë¦¬ìŠ¤í¬ ë¶„ì„ ê²°ê³¼
    
    # ì¢…í•© ë¶„ì„ í”„ë¡¬í”„íŠ¸ êµ¬ì„±
    msg = [
        HumanMessage(
            content=f"""ë‹¹ì‹ ì€ AI ìŠ¤íƒ€íŠ¸ì—…ì„ ìœ„í•œ ë²•ë¥  ë° ê·œì œ ì»¨ì„¤í„´íŠ¸ì…ë‹ˆë‹¤. {company}ì— ëŒ€í•œ ë‹¤ìŒ ë¶„ì„ì„ ë°”íƒ•ìœ¼ë¡œ ì¢…í•©ì ì¸ ë²•ì /ê·œì œ ê¶Œì¥ì‚¬í•­ì„ ì‘ì„±í•´ì£¼ì„¸ìš”:

ê¸°ì—…: {company}
ì‚°ì—…: {domain}
ì§€ì—­: {country}
ê¸°ìˆ  ìš”ì•½: {tech_summary}

ë²•ì  ë¶„ì„:
{legal_analysis}

ê¸°ìˆ  ë¦¬ìŠ¤í¬ ë¶„ì„:
{tech_analysis}

ë‹¤ìŒ í˜•ì‹ìœ¼ë¡œ ìµœì¢… ê¶Œì¥ì‚¬í•­ì„ ì‘ì„±í•´ì£¼ì„¸ìš” (ë°˜ë“œì‹œ í•œêµ­ì–´ë¡œ):

## {company} ë²•ì /ê·œì œ ë¦¬ìŠ¤í¬ ì¢…í•© í‰ê°€

### í•µì‹¬ ë¦¬ìŠ¤í¬ ìš”ì•½
(ì‚°ì—…, ê¸°ìˆ , ì§€ì—­ì„ ëª¨ë‘ ê³ ë ¤í•œ 3-5ê°€ì§€ í•µì‹¬ ë¦¬ìŠ¤í¬ ìš”ì•½)

### ë‹¨ê¸° ì¡°ì¹˜ì‚¬í•­ (0-6ê°œì›”)
(ì¦‰ê°ì ìœ¼ë¡œ ì·¨í•´ì•¼ í•  ë²•ì /ê·œì œ ì¤€ìˆ˜ ì¡°ì¹˜)

### ì¤‘ê¸° ëŒ€ì‘ ì „ëµ (6-18ê°œì›”)
(ê·œì œ ë³€í™”ì— ëŒ€ì‘í•˜ê¸° ìœ„í•œ ì¤‘ê¸° ì „ëµ)

### ì¥ê¸°ì  ê³ ë ¤ì‚¬í•­ (18ê°œì›” ì´ìƒ)
(ì¥ê¸°ì ì¸ ë²•ì /ê·œì œ í™˜ê²½ ë³€í™”ì— ëŒ€ë¹„í•˜ê¸° ìœ„í•œ ì „ëµ)

### ë§ì¶¤í˜• ë¦¬ìŠ¤í¬ ê´€ë¦¬ í”„ë ˆì„ì›Œí¬
(ì´ ê¸°ì—…ê³¼ ê¸°ìˆ ì— íŠ¹í™”ëœ ë¦¬ìŠ¤í¬ ê´€ë¦¬ ì ‘ê·¼ë²•)"""
        )
    ]
    
    # LLM ëª¨ë¸ë¡œ ì¢…í•© ë¶„ì„ ì‹¤í–‰
    model = ChatOpenAI(temperature=0, model=MODEL_NAME, streaming=True)
    response = model.invoke(msg)
    
    print("âœ… ì¢…í•© ë¶„ì„ ë° ê¶Œì¥ì‚¬í•­ ì™„ë£Œ")
    return {"messages": [response]}

# Agentic RAGë¥¼ ì‚¬ìš©í•œ ë²•ì /ê·œì œ ë¦¬ìŠ¤í¬ ë¶„ì„ ê·¸ë˜í”„ ìƒì„±
def create_legal_risk_graph():
    """ë²•ì  ë¦¬ìŠ¤í¬ ë¶„ì„ì„ ìœ„í•œ LangGraph ì›Œí¬í”Œë¡œìš° ìƒì„± í•¨ìˆ˜"""
    
    # ê·¸ë˜í”„ ì •ì˜
    workflow = StateGraph(LegalRiskAgentState)

    # ë…¸ë“œ ì •ì˜
    workflow.add_node("initial_query", initial_query)  # ì´ˆê¸° ì§ˆì˜ ì²˜ë¦¬
    workflow.add_node("pdf_retrieval", pdf_retrieval)  # PDF ë¬¸ì„œ ê²€ìƒ‰
    workflow.add_node("rewrite", rewrite)             # ì§ˆì˜ ì¬ì‘ì„±
    workflow.add_node("web_search", web_search)       # ì›¹ ê²€ìƒ‰
    workflow.add_node("analyze", analyze)             # ë²•ì  ë¶„ì„
    workflow.add_node("tech_risk_analysis", tech_risk_analysis)  # ê¸°ìˆ  íŠ¹í™” ë¦¬ìŠ¤í¬ ë¶„ì„ (ì¶”ê°€)
    workflow.add_node("comprehensive_analysis", comprehensive_analysis)  # ì¢…í•© ë¶„ì„ ë° ê¶Œì¥ì‚¬í•­ (ì¶”ê°€)
    workflow.add_node("legal_risks", analyze_legal_risks)  # ë¶„ì„ ê²°ê³¼ ì²˜ë¦¬

    # ì—£ì§€ ì •ì˜
    workflow.add_edge(START, "initial_query")
    workflow.add_edge("initial_query", "pdf_retrieval")

    # ì¡°ê±´ë¶€ ì—£ì§€: ê²€ìƒ‰ ê²°ê³¼ ê´€ë ¨ì„±ì— ë”°ë¼ ë¶„ê¸°
    workflow.add_conditional_edges(
        "pdf_retrieval",
        grade_documents,
        {
            "generate": "analyze",      # ê´€ë ¨ì„± ìˆìœ¼ë©´ ë¶„ì„
            "rewrite": "rewrite"        # ê´€ë ¨ì„± ì—†ìœ¼ë©´ ì¬ì‘ì„±
        }
    )

    # ì¬ì‘ì„± í›„ ì›¹ ê²€ìƒ‰ ì§„í–‰
    workflow.add_edge("rewrite", "web_search")
    
    # ì›¹ ê²€ìƒ‰ í›„ ë¶„ì„ ì§„í–‰
    workflow.add_edge("web_search", "analyze")
    
    # ë¶„ì„ í›„ ê¸°ìˆ  íŠ¹í™” ë¦¬ìŠ¤í¬ ë¶„ì„ ì§„í–‰ (ì¶”ê°€)
    workflow.add_edge("analyze", "tech_risk_analysis")
    
    # ê¸°ìˆ  ë¦¬ìŠ¤í¬ ë¶„ì„ í›„ ì¢…í•© ë¶„ì„ ë° ê¶Œì¥ì‚¬í•­ ì‘ì„± (ì¶”ê°€)
    workflow.add_edge("tech_risk_analysis", "comprehensive_analysis")
    
    # ì¢…í•© ë¶„ì„ í›„ ìµœì¢… ê²°ê³¼ ì²˜ë¦¬ (ìˆ˜ì •)
    workflow.add_edge("comprehensive_analysis", "legal_risks")
    
    # ë¶„ì„ ê²°ê³¼ ì²˜ë¦¬ ë° ì¢…ë£Œ
    workflow.add_edge("legal_risks", END)

    # ê·¸ë˜í”„ ì»´íŒŒì¼ ë° ë°˜í™˜
    return workflow.compile()

async def legal_risk_analysis_agent(company: str, domain: str, tech_summary: str, country: str):
    """ë²•ì  ë¦¬ìŠ¤í¬ ë¶„ì„ ì—ì´ì „íŠ¸ ì‹¤í–‰ í•¨ìˆ˜"""
    
    # ë²•ì /ê·œì œ ë¦¬ìŠ¤í¬ ê·¸ë˜í”„ ìƒì„±
    legal_graph = create_legal_risk_graph()
    
    # ì´ˆê¸° ìƒíƒœ ì„¤ì • (legal_assessments í•„ë“œ ì¶”ê°€)
    initial_state: LegalRiskAgentState = {
        "messages": [HumanMessage(content=f"{company}ì˜ ë²•ì  ê·œì œ ë¶„ì„")],
        "company": company,
        "domain": domain,
        "tech_summary": tech_summary,
        "country": country,
        "legal_assessments": {}  # ë¹ˆ ë”•ì…”ë„ˆë¦¬ë¡œ ì´ˆê¸°í™”
    }
    
    # ê·¸ë˜í”„ ì‹¤í–‰
    result = await legal_graph.ainvoke(initial_state)
    
    # ê²°ê³¼ ë°˜í™˜ (ìƒìœ„ ì‹œìŠ¤í…œì—ì„œ ì‚¬ìš©í•  ìˆ˜ ìˆë„ë¡)
    if "legal_assessments" in result and company in result["legal_assessments"]:
        return result["legal_assessments"][company]
    else:
        return "ë²•ì  í‰ê°€ë¥¼ ì™„ë£Œí•˜ì§€ ëª»í–ˆìŠµë‹ˆë‹¤."